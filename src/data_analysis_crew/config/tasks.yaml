# ---------------------------------------------------------------------------
#  1  LOAD ──────────────────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
load_data:
  description: >
    Load the CSV dataset using the exact *literal* path in `{dataset_path}`.
    ⚠️ Do **not** search or list directories.

    Use pandas to:
      • read the file  
      • `print()` shape and column names  
      • report missing-value counts and inferred dtypes  

    Dataset Path (string): **{dataset_path}**
  expected_output: >
    - Dataset shape  
    - `dtype_map` (column → dtype)  
    - `missing_values` (column → #missing)  
    - Path actually read/written
  agent: data_engineer

# ---------------------------------------------------------------------------
#  2  CLEAN ─────────────────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
clean_data:
  description: >
    Tidy up the raw dataset.

    • Call the tool:
      `load_or_clean(raw_path="{dataset_path}", cleaned_path="knowledge/diabetes_cleaned.csv")`
    • If the cleaned file exists, it’ll be re-used.
    • If **knowledge/diabetes_cleaned.csv** exists → just load it, gather
      metadata, and return – **do not** re-clean.  
    • Else → read **knowledge/diabetes.csv**, normalise column names
      (lower-snake-case), fix missing values / constants, then save to the
      same cleaned path.

    Always `print()` the cleaned file path as **the first line of STDOUT** so
    the Code-Interpreter tool captures it.

    Helper available:

    ```python
    from data_analysis_crew.tools import load_or_clean
    df = load_or_clean()            # ← re-uses cache or cleans afresh
    ```

    After cleaning assemble the `CleanedDataOutput` fields:
      • cleaned_path  
      • final_features / numeric_features / categorical_features  
      • dropped_columns / imputation_summary
  expected_output: >
    1  First STDOUT line → `knowledge/diabetes_cleaned.csv`  
    2  Markdown block:
       – cleaning steps  
       – final feature × dtype table  
       – post-clean missing-value summary
  agent: data_engineer
  context: [load_data]

# ---------------------------------------------------------------------------
#  3  EXPLORE ────────────────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
explore_data:
  description: >
    Perform an exploratory analysis of the cleaned dataset.

    Inputs (already in context):
      • Cleaned file → `cleaned_path`  
      • Numeric cols → `numeric_features`  
      • Categorical → `categorical_features`

    When you call the Code Interpreter, include
    {
      "code": "
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    from pathlib import Path

    df = pd.read_csv('knowledge/diabetes_cleaned.csv')

    # 1) Descriptive statistics
    stats = df.describe().to_string()
    print(stats)

    # 2) Example plot directory
    plots_dir = Path('output/plots'); plots_dir.mkdir(parents=True, exist_ok=True)

    # Histogram for each numeric column
    for col in df.select_dtypes('number').columns:
        df[col].hist()
        plt.title(col)
        filename = "{}_hist.png".format(col)
        plt.savefig(plots_dir / filename)
        plt.clf()

    # 3) Correlation heat-map
    plt.figure(figsize=(8,6))
    sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
    plt.tight_layout()
    heat_path = plots_dir / 'correlation_heatmap.png'
    plt.savefig(heat_path)
    print("Heat-map saved → {}".format(heat_path))

    # ALWAYS finish with a print so the tool
    # has something to capture as “final result”
    print('EDA script finished')
    ",
      "libraries_used": ["pandas", "matplotlib", "seaborn"]
    }.

    Tasks:
      1. Descriptive statistics (`df.describe()`; include categoricals)  
      2. Visualise distributions / relationships  
      3. Compute correlation matrix, list the top correlations with the target,
         flag outliers & anomalies.

    You **must** save **≥ 3** PNGs into **{output_dir}/plots/**.
  expected_output: >
    Markdown report with:
      • embeds / links for the saved plots  
      • table of top correlations  
      • notes on anomalies / biases / outliers
  agent: data_analyst
  context: [clean_data]

# ---------------------------------------------------------------------------
#  4  FEATURE SELECTION ──────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
select_features:
  description: >
    Select the strongest predictors for:

        {request}

    Use:
      • `top_correlations` (from EDA)  
      • `statistical_notes`

    Decide whether the prediction target is categorical or numeric:
      → return `"classification"` or `"regression"`.

    Deliver:
      • `top_features` – list & rationale  
      • `problem_type` – classification / regression (+ why)
  expected_output: >
    Markdown brief:
      – chosen features + justification  
      – inferred problem type + explanation
  agent: data_analyst
  context: [explore_data]

# ---------------------------------------------------------------------------
#  5  MODEL BUILDING ────────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
build_predictive_model:
  description: >
    Train a model on the cleaned dataset and save every artefact the
    dashboard needs.

    **Call the helper tool**  
      `build_predictive_model(df, out_dir="{output_dir}", problem_type=problem_type)`  
    that lives in **data_analysis_crew.tools**.

    The tool will:
      • train an appropriate model  
      • save `output/model-report.json`  
      • save plots to `output/plots/`  
      • write `output/final-insight-summary.md`

    Ensure the dashboard can find everything (all paths **relative**).

    Only hand-code training (i.e. build ML models), if the invoked tool does not work
    or does not lead to meaningful, data-related insights.
    Usually invoking the helper tool (build_predictive_model) is enough.

    Example for metrics:
    • If `problem_type == classification` → `RandomForestClassifier`  
      (report *accuracy* & *F1*).  
    • Else → `RandomForestRegressor`  
      (report *R²* & *MSE*).
    Be explicit and use metrics, if it makes sense. 

    Required outputs (all **relative paths**) in `{output_dir}`:

      ┌ output/model-report.json
      │   ├─ model_type
      │   ├─ target
      │   ├─ metrics {accuracy/F1 or R²/MSE}
      │   ├─ plain_summary      (one-liner for the dashboard)
      │   ├─ feature_importance_path   →  plots/feature_importances.png
      │   └─ confusion_matrix_path     →  plots/confusion_matrix.png
      └ output/plots/
          ├─ feature_importances.png
          └─ confusion_matrix.png      (only for classifiers)

    **Important**: create the `output/plots/` directory when missing.

    Also write a short Markdown block to `output/final-insight-summary.md`
    embedding the *relative* feature-importance image.
  expected_output: >
    - A brief Markdown summary with final metrics and the plot.
    - `output/model-report.json` (with metrics, paths, summary).  
    - `output/plots/feature_importances.png` (+ `confusion_matrix.png` if cls).
    - Short Markdown summary in `output/final-insight-summary.md` with all meaningful insight, metrics and plots.
  agent: model_builder
  context: [clean_data, select_features]
  output_file: output/model-report.json



# ---------------------------------------------------------------------------
#  6  EXECUTIVE SUMMARY ─────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
summarize_findings:
  description: >
    Craft an executive-level summary for non-technical stakeholders.

    Use:
      • `model_type`, `metrics`, `feature_importance_path`  
      • earlier narratives if useful.

    Must include:
      • 2-3 sentence plain-language overview  
      • 3-5 bullet key insights  
      • a one-sentence recommendation  
      • embedded feature-importance image:

          ![Feature Importances](`feature_importance_path`)
  expected_output: >
    Markdown file `output/final-insight-summary.md` with all of the above.
  agent: insight_reporter
  context: [build_predictive_model]
  output_file: output/final-insight-summary.md


# ---------------------------------------------------------------------------
#  7  LAUNCH DASHBOARD 🚀 ───────────────────────────────────────────────────
# ---------------------------------------------------------------------------
launch_dashboard:
  description: >
    Spin-up the Streamlit dashboard (dashboard.py) **after** the executive
    summary is written.  
    Steps  
      • run:  streamlit run dashboard.py  
      • wait 3 s, then open the browser on http://localhost:8501  
      • return a short confirmation string
  expected_output: >
    A message confirming the dashboard URL, e.g.:
    "Dashboard launched on http://localhost:8501"
  agent: data_project_manager
  context: [summarize_findings]