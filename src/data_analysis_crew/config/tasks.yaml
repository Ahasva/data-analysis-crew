# ---------------------------------------------------------------------------
#  1  LOAD ──────────────────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
load_data:
  description: >
    Load the CSV dataset using the exact *literal* path in `{dataset_path}`.
    ⚠️ Do **not** search or list directories.

    Use pandas to:
      • read the file  
      • `print()` shape and column names  
      • report missing-value counts and inferred dtypes  

    Dataset Path (string): **{dataset_path}**
  expected_output: >
    - Dataset shape  
    - `dtype_map` (column → dtype)  
    - `missing_values` (column → #missing)  
    - Path actually read/written
  agent: data_engineer

# ---------------------------------------------------------------------------
#  2  CLEAN ─────────────────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
clean_data:
  description: >
    Tidy up the raw dataset.

    • Call the tool:
      `load_or_clean(raw_path="{dataset_path}", cleaned_path="knowledge/diabetes_cleaned.csv")`
    • If the cleaned file exists, it’ll be re-used.
    • If **knowledge/diabetes_cleaned.csv** exists → just load it, gather
      metadata, and return – **do not** re-clean.  
    • Else → read **knowledge/diabetes.csv**, normalise column names
      (lower-snake-case), fix missing values / constants, then save to the
      same cleaned path.

    Always `print()` the cleaned file path as **the first line of STDOUT** so
    the Code-Interpreter tool captures it.

    Helper available:

    ```python
    from data_analysis_crew.tools import load_or_clean
    df = load_or_clean()            # ← re-uses cache or cleans afresh
    ```

    After cleaning assemble the `CleanedDataOutput` fields:
      • cleaned_path  
      • final_features / numeric_features / categorical_features  
      • dropped_columns / imputation_summary
  expected_output: >
    1  First STDOUT line → `knowledge/diabetes_cleaned.csv`  
    2  Markdown block:
       – cleaning steps  
       – final feature × dtype table  
       – post-clean missing-value summary
  agent: data_engineer
  context: [load_data]

# ---------------------------------------------------------------------------
#  3  EXPLORE ────────────────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
explore_data:
  description: >
    Perform an exploratory analysis of the cleaned dataset.

    Inputs (already in context):
      • Cleaned file → `cleaned_path`  
      • Numeric cols → `numeric_features`  
      • Categorical → `categorical_features`

    When you call the Code Interpreter, include
    {
      "code": "
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    from pathlib import Path

    df = pd.read_csv('knowledge/diabetes_cleaned.csv')

    # 1) Descriptive statistics
    stats = df.describe().to_string()
    print(stats)

    # 2) Example plot directory
    plots_dir = Path('output/plots'); plots_dir.mkdir(parents=True, exist_ok=True)

    # Histogram for each numeric column
    for col in df.select_dtypes('number').columns:
        df[col].hist()
        plt.title(col)
        filename = "{}_hist.png".format(col)
        plt.savefig(plots_dir / filename)
        plt.clf()

    # 3) Correlation heat-map
    plt.figure(figsize=(8,6))
    sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
    plt.tight_layout()
    heat_path = plots_dir / 'correlation_heatmap.png'
    plt.savefig(heat_path)
    print("Heat-map saved → {}".format(heat_path))

    # ALWAYS finish with a print so the tool
    # has something to capture as “final result”
    print('EDA script finished')
    ",
      "libraries_used": ["pandas", "matplotlib", "seaborn"]
    }.

    Tasks:
      1. Descriptive statistics (`df.describe()`; include categoricals)  
      2. Visualise distributions / relationships  
      3. Compute correlation matrix, list the top correlations with the target,
         flag outliers & anomalies.

    You **must** save **≥ 3** PNGs into **{output_dir}/plots/**.
  expected_output: >
    Markdown report with:
      • embeds / links for the saved plots  
      • table of top correlations  
      • notes on anomalies / biases / outliers
  agent: data_analyst
  context: [] # alternative: clean_data

# ---------------------------------------------------------------------------
#  4  FEATURE SELECTION ──────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
select_features:
  description: >
    Select the strongest predictors for:

        {request}

    Use:
      • `top_correlations` (from EDA)  
      • `statistical_notes`

    Decide whether the prediction target is categorical or numeric:
      → return `"classification"` or `"regression"`.

    Deliver:
      • `top_features` – list & rationale  
      • `problem_type` – classification / regression (+ why)
  expected_output: >
    Markdown brief:
      – chosen features + justification  
      – inferred problem type + explanation
  agent: data_analyst
  context: [explore_data]

# ---------------------------------------------------------------------------
#  5  MODEL BUILDING ────────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
build_predictive_model:
  description: >
    Train a predictive model using the cleaned dataset. 
    Save all required artefacts for downstream analysis and dashboard display.

    **Tool to call**:  
    `build_predictive_model(df, out_dir="{output_dir}", problem_type=problem_type)`

    ✅ You must choose an appropriate model:
      Available options:
        - "random_forest"
        - "logistic_reg"
        - "svm"
        - "knn"
        - "gbt"
        - "linear_reg"

      Example logic:
        - Logistic Regression → binary, low-dimensional problems  
        - Random Forest / GBT → non-linear, interpretable  
        - SVM → high feature count vs. samples  
        - KNN → small datasets  

    👉 Use `model_name="..."` explicitly in your call.

    📊 The tool will:
      - Train the model  
      - Save:
        • `output/model-report.json` (with type, target, metrics, plots, summary)  
        • `output/technical-metrics.md`  
        • Plots into `output/plots/`

    🛑 Do **not** hand-code model training unless the helper fails.

    Required output files (all relative):
      - `output/model-report.json`  
      - `output/plots/feature_importances.png`  
      - `output/plots/confusion_matrix.png` (only for classification)  
      - `output/technical-metrics.md`
  expected_output: >
    - `output/model-report.json` (with metrics, paths, summary).  
    - `output/plots/feature_importances.png` (+ `confusion_matrix.png` if cls).
    - `output/technical-metrics.md`
  agent: model_builder
  context: [clean_data, select_features]


# ---------------------------------------------------------------------------
#  6  EXECUTIVE SUMMARY ─────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
summarize_findings:
  description: >
    Craft an executive-level summary for non-technical stakeholders.

    Use:
      - `model_type`, `metrics`, `feature_importance_path`, 
      `output/technical-metrics.md` (from model_builder).
      - earlier narratives and outputs if helpful.

    Must include:
      - 2-3 sentence plain-language overview  
      - 3-5 bullet key insights  
      - a one-sentence recommendation  
      - embedded feature-importance image:
          ![Feature Importances](plots/feature_importances.png)
  expected_output: >
    Markdown file `output/final-insight-summary.md` with all of the above.
  agent: insight_reporter
  context: [build_predictive_model]
  output_file: output/final-insight-summary.md


# ---------------------------------------------------------------------------
#  7  LAUNCH DASHBOARD 🚀 ───────────────────────────────────────────────────
# ---------------------------------------------------------------------------
launch_dashboard:
  description: >
    Launch the Streamlit dashboard.

    Call this inside the Code Interpreter:
    ```python
    import subprocess, time, webbrowser

    subprocess.Popen(["streamlit", "run", "dashboard.py"])
    time.sleep(3)
    webbrowser.open("http://localhost:8501")
    print("Dashboard launched on http://localhost:8501")
    ```
  expected_output: >
    A message confirming the dashboard URL, e.g.:
    "Dashboard launched on http://localhost:8501"
  agent: insight_reporter
  context: [summarize_findings]