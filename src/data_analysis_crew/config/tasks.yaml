# ---------------------------------------------------------------------------
#  1  LOAD ──────────────────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
load_data:
  description: >
    Load the CSV dataset using the exact *literal* path in `{dataset_path}`.
    ⚠️ Do **not** search or list directories.

    Use pandas to:
      • read the file  
      • `print()` shape and column names  
      • report missing-value counts and inferred dtypes  

    Dataset Path (string): **{dataset_path}**
  expected_output: >
    - Dataset shape  
    - `dtype_map` (column → dtype)  
    - `missing_values` (column → #missing)  
    - Path actually read/written
  agent: data_engineer

# ---------------------------------------------------------------------------
#  2  CLEAN ─────────────────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
clean_data:
  description: >
    Tidy up the raw dataset.

    • Call the tool:
      `load_or_clean(raw_path="{dataset_path}", cleaned_path="knowledge/diabetes_cleaned.csv")`
    • If the cleaned file exists, it’ll be re-used.
    • If **knowledge/diabetes_cleaned.csv** exists → just load it, gather
      metadata, and return – **do not** re-clean.  
    • Else → read **knowledge/diabetes.csv**, normalise column names
      (lower-snake-case), fix missing values / constants, then save to the
      same cleaned path.

    Always `print()` the cleaned file path as **the first line of STDOUT** so
    the Code-Interpreter tool captures it.

    Helper available:

    ```python
    from data_analysis_crew.tools import load_or_clean
    df = load_or_clean()            # ← re-uses cache or cleans afresh
    ```

    After cleaning assemble the `CleanedDataOutput` fields:
      • cleaned_path  
      • final_features / numeric_features / categorical_features  
      • dropped_columns / imputation_summary
  expected_output: >
    1  First STDOUT line → `knowledge/diabetes_cleaned.csv`  
    2  Markdown block:
       – cleaning steps  
       – final feature × dtype table  
       – post-clean missing-value summary
  agent: data_engineer
  context: [load_data]

# ---------------------------------------------------------------------------
#  3  EXPLORE ────────────────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
explore_data:
  description: >
    Perform an exploratory analysis of the cleaned dataset.

    Inputs (already in context):
      • Cleaned file → `cleaned_path`  
      • Numeric cols → `numeric_features`  
      • Categorical → `categorical_features`

    Your job:
      - Describe the dataset statistically (e.g. `df.describe(include="all")`)
      - Visualize distributions of all numeric columns
      - Show relationships using a correlation heatmap
      - Save ≥3 high-quality plots to `{plot_path}/`
      - Print meaningful summary insights

    💡 You may also:
      - Investigate missing values, outliers, and collinearity
      - Add boxplots or pairplots if useful

    📊 Target plot expectations:
      {expected_plots}

    Tools and packages available:
      - pandas, seaborn, matplotlib, pathlib
      - `CodeInterpreterTool` will handle execution

    📁 Make sure this folder exists before saving plots:
    ```python
    from pathlib import Path
    Path("{plot_path}").mkdir(parents=True, exist_ok=True)
    ```

    Finish with a markdown summary of your findings, including embedded images and key correlations.
  expected_output: >
    Markdown report with:
      - 2–5 image embeds (use <img src="…" width="600" />)
      - table of strongest correlations
      - 3–5 bullet summary insights
  agent: data_analyst
  context: [clean_data]

# ---------------------------------------------------------------------------
#  4  FEATURE SELECTION ──────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
select_features:
  description: >
    Based on the goal:

        {request}

    and the prior EDA results, identify the strongest predictive features and
    determine whether this is a **classification** or **regression** problem.

    You have access to:
      - `top_correlations` (from the correlation matrix)
      - Descriptive statistics and variable types
      - Anomalies, distributions, missing values
      - consider which metrics would be useful:
        - classification problem: {classification_metrics}
        - regression problem: {regression_metrics} 

    Your tasks:
      1. Identify the most relevant features for modeling — explain your reasoning
      2. Determine the appropriate problem type:
         - classification → predict categories (e.g. disease = yes/no)
         - regression → predict numeric values (e.g. blood sugar level)
      3. Justify your decision using data context + field descriptions

    ❗ Do **not** hard-code model names yet — selection happens later.

    You may optionally suggest models from this list: 
      - classification_models: {classification_models}, 
      - regression models: {regression_models}, 

    Return:
      - `top_features` – list of chosen columns, 
      - `problem_type` – `"classification"` or `"regression"`, 
      - explanation why, 
  expected_output: >
    Markdown brief with:
      - Chosen features + justification (refer to correlation, distributions, domain knowledge)
      - Inferred problem type + explanation
      - Optional: brief note on suitable model families
  agent: data_analyst
  context: [explore_data]

# ---------------------------------------------------------------------------
#  5  MODEL BUILDING ────────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
build_predictive_model:
  description: >
    Train multiple predictive models using the cleaned dataset, 
    compare them, and select the best based on validation performance.

    🛠️ Tool to call:
    `build_predictive_model(data=cleaned_path, out_dir="{output_dir}", problem_type=problem_type, tuning=True)`

    ✅ This tool:
      - Automatically tries multiple model types depending on the problem (classification or regression)
      - Applies hyperparameter tuning with GridSearchCV (`tuning=True`)
      - Selects the best model using F1 or R²
      - Saves:
          • `output/model-report.json` with type, metrics, and summary  
          • `output/technical-metrics.md`  
          • visualizations into `{plot_path}/`  
      - Exposes `all_model_scores` for dashboard comparisons

    📌 Scope of models to consider:
      - classificationmodels: {classification_models}, 
      - regression models: {regression_models}, 

    📊 Evaluation metrics (choose per problem type):
      - classification problem: {classification_metrics}, 
      - regression problem: {regression_metrics},
    📝 Use the most relevant metrics. The lists below are suggestions:

    🖼️ Ensure the following plots are saved (if relevant):
      {expected_plots}
    💡 If a plot is not applicable (e.g., ROC for regression), skip gracefully.

    🛑 Do **not** hard-code or guess the model name. The tool handles selection.

    For all embedded plots in markdown, use:
    <img src="plots/feature_importances.png" width="600" />
  expected_output: >
    - `output/model-report.json` with selected model, metrics, summary, paths
    - `output/plots/feature_importances.png` (+ confusion_matrix.png if classification)  
    - `output/technical-metrics.md`
  agent: model_builder
  context: [clean_data, select_features]

# ---------------------------------------------------------------------------
#  6  EXECUTIVE SUMMARY ─────────────────────────────────────────────────────
# ---------------------------------------------------------------------------
summarize_findings:
  description: >
    Craft an executive-level summary for non-technical stakeholders.

    Use:
      - `model_type`, `metrics`, `feature_importance_path`,
      - earlier narratives and outputs if helpful.

    Include:
      - A short overview of what the data shows and what was done
      - 3–5 key takeaways in bullet form
      - A one-sentence recommendation (e.g., "use this model", "investigate this feature", etc.)
      - An embedded image of the top feature importances:
          <img src="plots/feature_importances.png" width="600" />

    📊 Expected focus:
      - High-impact metrics
        - classification problem: {classification_metrics}
        - regression problem: {regression_metrics}
      - Mention if additional plots (like {expected_plots}) helped shape your conclusion

    🛠️ Use plain, stakeholder-friendly language.
  expected_output: >
    Markdown file `output/final-insight-summary.md` containing:

    ## ✅ Executive Summary
    - 2–3 sentence narrative overview of what the data reveals
    - What analysis was performed and what model was selected and why it was selected

    ## ✅ Key Insights
    - 3–5 bullet points summarizing important findings, correlations, or warnings

    ## ✅ Recommendation
    - Clear, actionable next step

    ## ✅ Embedded Visuals
    - At least one image embed like:
      <img src="plots/feature_importances.png" width="600" />
    - Mention if additional plots were considered from: {expected_plots}

    ## ✅ Metrics Summary
    - Use either classification or regression metrics:
      - classification: {classification_metrics}
      - regression: {regression_metrics}

    ## ✅ Final Checklist
    - Summary stats ✔  
    - Key insight bullets ✔  
    - Final model + metrics ✔  
    - Embedded visual ✔  
    - Dashboard launched ✔  
  agent: insight_reporter
  context: [build_predictive_model]
  output_file: output/final-insight-summary.md

# ---------------------------------------------------------------------------
#  7  VALIDATE SUMMARY CHECKLIST ✅
# ---------------------------------------------------------------------------
validate_summary:
  description: >
    Run a final checklist audit on the summary report `output/final-insight-summary.md`.

    Use FileReadTool to open and audit: `output/final-insight-summary.md`.

    Confirm presence of:
      - Executive summary section
      - 3–5 bullet insights
      - Embedded image(s)
      - One or more model metrics
      - Mention of the dashboard or application launch

    For each, mark whether it's ✅ present or ❌ missing.  
    Provide short suggestions if any part is missing.

    Also use FileWriterTool to save a short log file:
      - Filename (parameter: `filename`): `summary_qc_log.txt`
      - Content (parameter: `content`) should include:
        - Timestamp of validation
        - Checklist pass/fail results
        - A one-line summary verdict ("✅ Passed" or "❌ Failed")
      - Directory (parameter: `directory`): `output/`

    📎 Refer to earlier agent outputs if needed.

  expected_output: >
    A markdown checklist report confirming each item, e.g.:
    ```
    ## Final Report Checklist
    - ✅ Executive summary present
    - ✅ Bullet insights included
    - ✅ Model metrics mentioned
    - ❌ Dashboard not mentioned → Add a sentence about user access
    ```
    ➕ Log file: `output/summary_qc_log.txt` should be created with timestamp and pass/fail.
  agent: quality_checker
  context: [summarize_findings]
  output_file: output/final-report-checklist.md

# ---------------------------------------------------------------------------
#  8  LAUNCH DASHBOARD 🚀 ───────────────────────────────────────────────────
# ---------------------------------------------------------------------------
launch_dashboard:
  description: >
    Launch the Streamlit dashboard by calling the prebuilt tool:
    `launch_dashboard(path="dashboard.py", port=8501)`

    The tool will:
      • Start the dashboard server via Streamlit
      • Open the browser to the correct local URL
      • Print confirmation that the dashboard is live
  expected_output: >
    A message confirming the dashboard URL, e.g.:
    "Dashboard launched on http://localhost:8501"
  agent: insight_reporter
  context: [validate_summary]
  