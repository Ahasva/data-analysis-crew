# ============================================================================
#  DATA ANALYSIS CREW - TASK DEFINITIONS (cleaned & path-corrected)
# ============================================================================

# 1. LOAD RAW CSV (Prompt-based, no custom tool)
load_data:
  description: >
    Load the dataset from `{raw_path}` and return basic metadata inside folder called `output/`, like:
    - DataFrame shape (rows, columns)
    - List of column names
    - Dictionary of missing values per column
    - Dictionary of data types per column

    Use Python and pandas to compute this information (via CodeInterpreterTool), example:
    ```python
    import pandas as pd
    df = pd.read_csv("{dataset_path}")
    print(df.shape)
    print(list(df.columns))
    print(df.isnull().sum().to_dict())
    print(df.dtypes.astype(str).to_dict())
    ```

    Your output must be returned as structured data, preferably CSV, 
    that downstream agents can parse.

    Required input:
    - raw_path
  expected_output: >
    ```json
    {
      "shape": [rows, columns],
      "columns": [...],
      "missing": {...},
      "dtypes": {...}
    }
    ```
  agent: data_engineer
  output_file: output/{file_name}_metadata.json

# 2. CLEAN DATA (Prompt-driven with Docker-based code interpreter)
clean_data:
  description: >
    Clean the dataset at `{raw_path}`!
    - Load the CSV using pandas
    - Normalize column names (lowercase, replace spaces with underscores)
    - Handle missing values intelligently (drop or fill)
    - Ensure the directory for `{cleaned_path}` exists: {root_folder}
    - Save the cleaned file to `{cleaned_path}` by calling:
      ```python
      df.to_csv(cleaned_path, index=False)
      ```
    - Return a fully cleaned dataset that can be used for machine learning modeling
    - Return a confirmation message like: "Cleaned data saved to {cleaned_path}"
    
    Return:
    - Path to the cleaned file
    - Lists of final, numeric, and categorical features
    - Optional: JSON summary of shape, final_featureds, numeric_features, 
    categorical_features, column_names, missing values handled

  expected_output: >
    - **MOST IMPORTANT**: fully cleaned dataset (CSV) based on {raw_path} saved to {cleaned_path}
    - Secondary: JSON file with a structure like:
      ```json
      {
      "cleaned_path": "{cleaned_path}",
      "final_features": [...],
      "numeric_features": [...],
      "categorical_features": [...],
      "summary_markdown": "..."
      }
      ```
  agent: data_engineer
  context: [load_data]
  output_file: knowledge/{file_name}_cleaned.csv

# 3. EXPLORE DATA
explore_data:
  description: >
    Perform an exploratory analysis of the cleaned dataset provided by the `clean_data` task!
    - Summarize the dataset using `df.describe(include="all")` and `df.info(verbose=True)`
    - Visualize distributions of numeric features
    - Generate a correlation heatmap
    - Identify outliers, collinearity, and missing values
    - Use approved libraries from: {available_libraries}; use other libraries, if necessary
    - Save at least 3 visualizations (e.g., heatmap, histograms) to folder structure `{plot_path}`: `output/plots/...`.

    Notes:
    - Focus on key statistical takeaways
    - Ensure plots are informative for model-building
    - Use built-in `explore_data` tool if available

  expected_output: >
    - Markdown report with:
      • 2–5 embedded plots (`<img src="..."/>`)
      • Top correlations with the target variable
      • Bullet point summary (3–5 key findings)
  agent: data_analyst
  context: [clean_data]

# 4. SELECT FEATURES
select_features:
  description: >
    Based on the user's request `{request}`, determine:

    - The appropriate problem type (classification or regression)
    - The best features to use for modeling
    - A suitable target variable

    Consider:
    - Feature correlations, outliers, distributions, missingness, descriptive statistics, 
    variable type, anomalies, distributions, missing values etc.
    - Metrics and model families from:
      - Classification: {classification_metrics}
      - Regression: {regression_metrics}

  expected_output: >
    ```json
    {
      "top_features": [...],
      "problem_type": "classification",
      "target_variable": "...",
      "explanation": "..."
    }
    ```
  agent: data_analyst
  context: [explore_data]

# 5. BUILD MODEL
build_predictive_model:
  description: >
    Train and evaluate Machine Learning models on the cleaned dataset produced by the `clean_data` task!
    - Auto-detect or confirm problem type (classification/regression)
    - Train and tune models from `{available_models}`
    - Save all visual outputs to `{plot_path}` (e.g. `output/plots/...`)
    - Save evaluation summaries to `{output_dir}` as `.json` and `.md`

    Use `build_predictive_model` tool if available.

  expected_output: >
    - JSON report at `output/model-report.json`
    - Plots at {plot_path} such as `feature_importance.png`, `confusion_matrix.png`, `residuals.png`, e.g `output/plots/feature_importance.png`
    - Markdown summary: `output/technical-metrics.md`

  agent: model_builder
  context: [clean_data, select_features]

# 6. SUMMARY REPORT
summarize_findings:
  description: >
    Create an executive-level summary of the modeling results.

    - Summarize the selected model and its performance
    - Embed plots (at least one, e.g. `<img src="..." width="480"/>`)
    - Highlight 3–5 key insights
    - Make a recommendation

    Use metrics from:
      - Classification: {classification_metrics}
      - Regression: {regression_metrics}

  expected_output: >
    A Markdown report saved as `output/final-insight-summary.md`, with:
    - Executive summary (at least 2–3 sentence narrative overview of what the data reveals; 
    what analysis was performed; what model was selected and why it was selected)
    - Key insights (at least 3-5 bullet points, which summarize important findings, correlations, etc.)
    - Recommendation (clear, actionable next steps)
    - Embedded plots (use relative paths)
    - Metrics summary table
    (Use a header structure: ## ✅)
  agent: insight_reporter
  context: [build_predictive_model]
  output_file: output/final-insight-summary.md

# 7. CHECKLIST VALIDATION
validate_summary:
  description: >
    Run a checklist audit on `output/final-insight-summary.md`:

    - Confirm all required sections are present
    - Flag missing components (plots, metrics, summaries) for each, mark ✅ present or ❌ missing
    - Save results to `output/summary_qc_log.txt` 

    Include timestamp, pass/fail flags, and fix suggestions.
  expected_output: >
    - Markdown checklist report (e.g. ✅ Bullet insights included)
    - Checklist saved as `output/summary_qc_log.txt`
  agent: quality_checker
  context: [summarize_findings]
  output_file: output/final-report-checklist.md

# 8. LAUNCH DASHBOARD
launch_dashboard:
  description: >
    Launch the dashboard using the tool `launch_dashboard`.

    Do not open or edit `{dashboard_file}`.
    Simply run: `launch_dashboard(path="{dashboard_file}", port=8501)` after all other tasks finished

  expected_output: >
    "Dashboard launched on http://localhost:8501"

  agent: insight_reporter
  context: [validate_summary]
  