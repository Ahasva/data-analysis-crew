# ---------------------------------------------------------------------------
#  1  LOAD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
load_data:
  description: >
    Load the CSV dataset using the exact *literal* path in `{dataset_path}`.
    âš ï¸ Do **not** search or list directories.

    Use pandas to:
      â€¢ read the file  
      â€¢ `print()` shape and column names  
      â€¢ report missing-value counts and inferred dtypes  

    Dataset Path (string): **{dataset_path}**
  expected_output: >
    - Dataset shape  
    - `dtype_map` (column â†’ dtype)  
    - `missing_values` (column â†’ #missing)  
    - Path actually read/written
  agent: data_engineer

# ---------------------------------------------------------------------------
#  2  CLEAN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
clean_data:
  description: >
    Tidy up the raw dataset.

    â€¢ Call the tool:
      `load_or_clean(raw_path="{dataset_path}", cleaned_path="knowledge/diabetes_cleaned.csv")`
    â€¢ If the cleaned file exists, itâ€™ll be re-used.
    â€¢ If **knowledge/diabetes_cleaned.csv** exists â†’ just load it, gather
      metadata, and return â€“ **do not** re-clean.  
    â€¢ Else â†’ read **knowledge/diabetes.csv**, normalise column names
      (lower-snake-case), fix missing values / constants, then save to the
      same cleaned path.

    Always `print()` the cleaned file path as **the first line of STDOUT** so
    the Code-Interpreter tool captures it.

    Helper available:

    ```python
    from data_analysis_crew.tools import load_or_clean
    df = load_or_clean()            # â† re-uses cache or cleans afresh
    ```

    After cleaning assemble the `CleanedDataOutput` fields:
      â€¢ cleaned_path  
      â€¢ final_features / numeric_features / categorical_features  
      â€¢ dropped_columns / imputation_summary
  expected_output: >
    1  First STDOUT line â†’ `knowledge/diabetes_cleaned.csv`  
    2  Markdown block:
       â€“ cleaning steps  
       â€“ final feature Ã— dtype table  
       â€“ post-clean missing-value summary
  agent: data_engineer
  context: [load_data]

# ---------------------------------------------------------------------------
#  3  EXPLORE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
explore_data:
  description: >
    Perform an exploratory analysis of the cleaned dataset.

    Inputs (already in context):
      â€¢ Cleaned file â†’ `cleaned_path`  
      â€¢ Numeric cols â†’ `numeric_features`  
      â€¢ Categorical â†’ `categorical_features`

    Your job:
      - Describe the dataset statistically (e.g. `df.describe(include="all")`)
      - Visualize distributions of all numeric columns
      - Show relationships using a correlation heatmap
      - Save â‰¥3 high-quality plots to `{plot_path}/`
      - Print meaningful summary insights

    ğŸ’¡ You may also:
      - Investigate missing values, outliers, and collinearity
      - Add boxplots or pairplots if useful

    ğŸ“Š Target plot expectations:
      {expected_plots}

    Tools and packages available:
      - pandas, seaborn, matplotlib, pathlib
      - `CodeInterpreterTool` will handle execution

    ğŸ“ Make sure this folder exists before saving plots:
    ```python
    from pathlib import Path
    Path("{plot_path}").mkdir(parents=True, exist_ok=True)
    ```

    Finish with a markdown summary of your findings, including embedded images and key correlations.
  expected_output: >
    Markdown report with:
      - 2â€“5 image embeds (use <img src="â€¦" width="600" />)
      - table of strongest correlations
      - 3â€“5 bullet summary insights
  agent: data_analyst
  context: [clean_data]

# ---------------------------------------------------------------------------
#  4  FEATURE SELECTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
select_features:
  description: >
    Based on the goal:

        {request}

    and the prior EDA results, identify the strongest predictive features and
    determine whether this is a **classification** or **regression** problem.

    You have access to:
      - `top_correlations` (from the correlation matrix)
      - Descriptive statistics and variable types
      - Anomalies, distributions, missing values
      - consider which metrics would be useful:
        - classification problem: {classification_metrics}
        - regression problem: {regression_metrics} 

    Your tasks:
      1. Identify the most relevant features for modeling â€” explain your reasoning
      2. Determine the appropriate problem type:
         - classification â†’ predict categories (e.g. disease = yes/no)
         - regression â†’ predict numeric values (e.g. blood sugar level)
      3. Justify your decision using data context + field descriptions

    â— Do **not** hard-code model names yet â€” selection happens later.

    You may optionally suggest models from this list: 
      - classification_models: {classification_models}, 
      - regression models: {regression_models}, 

    Return:
      - `top_features` â€“ list of chosen columns, 
      - `problem_type` â€“ `"classification"` or `"regression"`, 
      - explanation why, 
  expected_output: >
    Markdown brief with:
      - Chosen features + justification (refer to correlation, distributions, domain knowledge)
      - Inferred problem type + explanation
      - Optional: brief note on suitable model families
  agent: data_analyst
  context: [explore_data]

# ---------------------------------------------------------------------------
#  5  MODEL BUILDING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
build_predictive_model:
  description: >
    Train multiple predictive models using the cleaned dataset, 
    compare them, and select the best based on validation performance.

    ğŸ› ï¸ Tool to call:
    `build_predictive_model(data=cleaned_path, out_dir="{output_dir}", problem_type=problem_type, tuning=True)`

    âœ… This tool:
      - Automatically tries multiple model types depending on the problem (classification or regression)
      - Applies hyperparameter tuning with GridSearchCV (`tuning=True`)
      - Selects the best model using F1 or RÂ²
      - Saves:
          â€¢ `output/model-report.json` with type, metrics, and summary  
          â€¢ `output/technical-metrics.md`  
          â€¢ visualizations into `{plot_path}/`  
      - Exposes `all_model_scores` for dashboard comparisons

    ğŸ“Œ Scope of models to consider:
      - classificationmodels: {classification_models}, 
      - regression models: {regression_models}, 

    ğŸ“Š Evaluation metrics (choose per problem type):
      - classification problem: {classification_metrics}, 
      - regression problem: {regression_metrics},
    ğŸ“ Use the most relevant metrics. The lists below are suggestions:

    ğŸ–¼ï¸ Ensure the following plots are saved (if relevant):
      {expected_plots}
    ğŸ’¡ If a plot is not applicable (e.g., ROC for regression), skip gracefully.

    ğŸ›‘ Do **not** hard-code or guess the model name. The tool handles selection.

    For all embedded plots in markdown, use:
    <img src="plots/feature_importances.png" width="600" />
  expected_output: >
    - `output/model-report.json` with selected model, metrics, summary, paths
    - `output/plots/feature_importances.png` (+ confusion_matrix.png if classification)  
    - `output/technical-metrics.md`
  agent: model_builder
  context: [clean_data, select_features]

# ---------------------------------------------------------------------------
#  6  EXECUTIVE SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
summarize_findings:
  description: >
    Craft an executive-level summary for non-technical stakeholders.

    Use:
      - `model_type`, `metrics`, `feature_importance_path`,
      - earlier narratives and outputs if helpful.

    Include:
      - A short overview of what the data shows and what was done
      - 3â€“5 key takeaways in bullet form
      - A one-sentence recommendation (e.g., "use this model", "investigate this feature", etc.)
      - An embedded image of the top feature importances:
          <img src="plots/feature_importances.png" width="600" />

    ğŸ“Š Expected focus:
      - High-impact metrics
        - classification problem: {classification_metrics}
        - regression problem: {regression_metrics}
      - Mention if additional plots (like {expected_plots}) helped shape your conclusion

    ğŸ› ï¸ Use plain, stakeholder-friendly language.
  expected_output: >
    Markdown file `output/final-insight-summary.md` containing:

    ## âœ… Executive Summary
    - 2â€“3 sentence narrative overview of what the data reveals
    - What analysis was performed and what model was selected and why it was selected

    ## âœ… Key Insights
    - 3â€“5 bullet points summarizing important findings, correlations, or warnings

    ## âœ… Recommendation
    - Clear, actionable next step

    ## âœ… Embedded Visuals
    - At least one image embed like:
      <img src="plots/feature_importances.png" width="600" />
    - Mention if additional plots were considered from: {expected_plots}

    ## âœ… Metrics Summary
    - Use either classification or regression metrics:
      - classification: {classification_metrics}
      - regression: {regression_metrics}

    ## âœ… Final Checklist
    - Summary stats âœ”  
    - Key insight bullets âœ”  
    - Final model + metrics âœ”  
    - Embedded visual âœ”  
    - Dashboard launched âœ”  
  agent: insight_reporter
  context: [build_predictive_model]
  output_file: output/final-insight-summary.md

# ---------------------------------------------------------------------------
#  7  VALIDATE SUMMARY CHECKLIST âœ…
# ---------------------------------------------------------------------------
validate_summary:
  description: >
    Run a final checklist audit on the summary report `output/final-insight-summary.md`.

    Use FileReadTool to open and audit: `output/final-insight-summary.md`.

    Confirm presence of:
      - Executive summary section
      - 3â€“5 bullet insights
      - Embedded image(s)
      - One or more model metrics
      - Mention of the dashboard or application launch

    For each, mark whether it's âœ… present or âŒ missing.  
    Provide short suggestions if any part is missing.

    Also use FileWriterTool to save a short log file:
      - Filename (parameter: `filename`): `summary_qc_log.txt`
      - Content (parameter: `content`) should include:
        - Timestamp of validation
        - Checklist pass/fail results
        - A one-line summary verdict ("âœ… Passed" or "âŒ Failed")
      - Directory (parameter: `directory`): `output/`

    ğŸ“ Refer to earlier agent outputs if needed.

  expected_output: >
    A markdown checklist report confirming each item, e.g.:
    ```
    ## Final Report Checklist
    - âœ… Executive summary present
    - âœ… Bullet insights included
    - âœ… Model metrics mentioned
    - âŒ Dashboard not mentioned â†’ Add a sentence about user access
    ```
    â• Log file: `output/summary_qc_log.txt` should be created with timestamp and pass/fail.
  agent: quality_checker
  context: [summarize_findings]
  output_file: output/final-report-checklist.md

# ---------------------------------------------------------------------------
#  8  LAUNCH DASHBOARD ğŸš€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
launch_dashboard:
  description: >
    Launch the Streamlit dashboard by calling the prebuilt tool:
    `launch_dashboard(path="dashboard.py", port=8501)`

    The tool will:
      â€¢ Start the dashboard server via Streamlit
      â€¢ Open the browser to the correct local URL
      â€¢ Print confirmation that the dashboard is live
  expected_output: >
    A message confirming the dashboard URL, e.g.:
    "Dashboard launched on http://localhost:8501"
  agent: insight_reporter
  context: [validate_summary]
  