# ============================================================================
#  DATA ANALYSIS CREW - TASK DEFINITIONS (cleaned & path-corrected)
# ============================================================================

# 1. LOAD RAW CSV
load_data:
  description: >
    TASK:
    Load the CSV file at {dataset_path} and summarize the data.

    template_variables: [dataset_path]
    
    tool_input:
      dataset_path: {dataset_path}

    Use Python (via CodeInterpreterTool), example:
    ```python
    import pandas as pd
    df = pd.read_csv("{dataset_path}")
    print(df.shape)
    print(list(df.columns))
    print(df.isnull().sum().to_dict())
    print(df.dtypes.astype(str).to_dict())
    ```
    
  expected_output: >
    - Tuple shape (rows, columns)  
    - List of columns  
    - Dict of missing values  
    - Dict of column dtypes  
  agent: data_engineer
  

# 2. CLEAN DATA
clean_data:
  description: >
    TASK:
    Clean the dataset!:
    - Normalize column names
    - Handle missing values
    - Save to {cleaned_path}

    template_variables: [raw_path, cleaned_path]

    tool_input:
      raw_path: {raw_path}
      cleaned_path: {cleaned_path}

    If file exists, reuse. Else, clean from {raw_path}.

    Tool:
    ```python
    load_or_clean(raw_path="{raw_path}", cleaned_path="{cleaned_path}")
    ```
  expected_output: >
    1. First line: the cleaned file path  
    2. Markdown summary of changes, features, types, and missing values
  agent: data_engineer
  context: [load_data]
  

# 3. EXPLORE DATA
explore_data:
  description: >
    TASK:
    Perform an exploratory analysis of the cleaned dataset: Analyze and visualize the dataset at {cleaned_path}:
    - Describe the dataset statistically (e.g. `df.describe(include="all")`), 
    - Visualize distributions of all numeric columns, 
    - Show relationships using a correlation heatmap, 
    - Investigate missing values, outliers, and collinearity, 
    - Add boxplots or pairplots if useful, 
    - Save ≥{expected_plots} plots to {plot_path}, 
    - Summarize key insights in Markdown, 
    - Save ≥3 high-quality plots to {plot_path} (e.g. output/plots/*.png), 
    - Use typical data science and machine learning libraries, e.g.: {available_libraries},
    Finish with a markdown summary of your findings, including embedded images and key correlations.

    📁 Make sure this folder exists before saving plots:
    ```python
    from pathlib import Path
    Path("{plot_path}").mkdir(parents=True, exist_ok=True)
    ```

    template_variables:
      [cleaned_path, numeric_features, categorical_features, plot_path, expected_plots]
    
    tool_input:
      cleaned_path: {cleaned_path}
      numeric_features: {numeric_features}
      plot_path: {plot_path}
      expected_plots: {expected_plots}
  expected_output: >
    Markdown report with:
      - 2–5 image embeds (use <img src="…" width="480" />)
      - table of strongest correlations
      - 3–5 bullet summary insights
  agent: data_analyst
  context: [clean_data]


# 4. SELECT FEATURES
select_features:
  description: >
    TASK:
    Pick the best features and problem type based on the user's request: {request}.

    Consider:
      - {classification_metrics} vs {regression_metrics}
      - top_correlations, outliers, and missingness, descriptive statistics,
      variable type, anomalies, distributions, missing values etc. (data driven approach)

    template_variables:
      [request, numeric_features, categorical_features, classification_models,
      regression_models, classification_metrics, regression_metrics]
  expected_output: >
    {
      "top_features": [...],
      "problem_type": "classification",
      "target_variable": "...",
      "explanation": "..."
    }
  agent: data_analyst
  context: [explore_data]
  

# 5. BUILD MODEL
build_predictive_model:
  description: >
    TASK:
    Train multiple models on {cleaned_path} (to be found in {root_folder}).
    If it is a labeled dataset, find for "target variable" yourself. It is sometimes called "output" or "y label" etc.
    - Save files like .md or .json to {output_dir} and visuals of metrics (images like .png) to {plot_path}.
    - Auto-tune & compare models (based on {available_models}).

    template_variables:
      [root_folder, cleaned_path, output_dir, plot_path, available_models,
      classification_models, regression_models, classification_metrics,
      regression_metrics, expected_plots]

    tool_input:
      root_folder: {root_folder}
      data: {cleaned_path}
      available_models: {available_models}
      out_dir: {output_dir}
      tuning: "true"
  expected_output: >
    - `output/model-report.json` with selected model, metrics, summary, paths
    - `output/plots/feature_importances.png` (e.g confusion_matrix.png, residuals.png)  
    - `output/technical-metrics.md`
  agent: model_builder
  context: [clean_data, select_features]
  

# 6. SUMMARY REPORT
summarize_findings:
  description: >
    TASK:
    Craft an executive-level summary for non-technical stakeholders.
    Summarize the modeling results with metrics and visuals. Focus on high-impact metrics.

    template_variables:
      [model_type, metrics, feature_importance_path, classification_metrics,
      regression_metrics, expected_plots]
    
    Embed plot:  
    `<img src="plots/feature_importances.png" width="480" />`
  expected_output: >
    Markdown file `output/final-insight-summary.md` containing:

    ## ✅ Executive Summary
    - 2–3 sentence narrative overview of what the data reveals
    - What analysis was performed and what model was selected and why it was selected

    ## ✅ Key Insights
    - 3–5 bullet points summarizing important findings, correlations, or warnings

    ## ✅ Recommendation
    - Clear, actionable next step

    ## ✅ Embedded Visuals
    - At least one image embed like:
      <img src="plots/feature_importances.png" width="480" />
    - Mention if additional plots were considered from: {expected_plots}

    ## ✅ Metrics Summary
    - Use either classification or regression metrics:
      - classification: {classification_metrics}
      - regression: {regression_metrics}
  agent: insight_reporter
  context: [build_predictive_model]
  output_file: output/final-insight-summary.md

# 7. CHECKLIST VALIDATION
validate_summary:
  description: >
    TASK:
    Run a final checklist audit on the summary report `output/final-insight-summary.md`.
    Save results to `output/summary_qc_log.txt`.
    - For each, mark ✅ present or ❌ missing and add a brief suggestion if missing.
    
    - After the audit, write a log file `summary_qc_log.txt` into the `output/` folder
    (use FileWriterTool). The log should include timestamp, pass/fail flags, and a final verdict.
    - Confirm presence of:
      • Executive summary section  
      • 3–5 bullet insights  
      • Embedded image(s)  
      • One or more model metrics  
      • Mention of the dashboard or application launch 
    
    template_variables: [expected_plots]
  expected_output: >
    A markdown checklist report confirming each item, e.g.:
    ```
    ## Final Report Checklist
    - ✅ Executive summary present
    - ✅ Bullet insights included
    - ✅ Model metrics mentioned
    - ❌ Dashboard not mentioned → Add a sentence about user access
    ```
    - `output/summary_qc_log.txt` with timestamp and pass/fail
  agent: quality_checker
  context: [summarize_findings]
  output_file: output/final-report-checklist.md

# 8. LAUNCH DASHBOARD
launch_dashboard:
  description: >
    TASK:
    Launch the dashboard with the tool launch_dashboard:
    `launch_dashboard(path="dashboard.py", port=8501)`

    This tool launches the dashboard. Do not interpret this task as code generation or file writing.
    It must use the existing Streamlit script located at: {dashboard_file}.

    Do not open `./{dashboard_file}`! Do not overwrite `./{dashboard_file}`!

    template_variables: [dashboard_file, port]
    tool_input:
      path: {dashboard_file}
      port: "8501"    
  expected_output: >
    "Dashboard launched on http://localhost:8501"
  agent: insight_reporter
  context: [validate_summary]
  